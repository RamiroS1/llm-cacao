"""
M√≥dulo LLaVA Optimizado para Dr. agro
Especialidad: Detecci√≥n de Enfermedades en Plantas
Caracter√≠sticas:
- Formato de salida Markdown
- Filtro r√°pido de conversaci√≥n 
- Gesti√≥n manual de memoria VRAM
"""

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration
from PIL import Image
import gc
import re

class LlavaPlantDiseaseDetector:
    def __init__(self, model_id="YuchengShi/LLaVA-v1.5-7B-Plant-Leaf-Diseases-Detection"):
        """
        Inicializa el detector con configuraci√≥n para respuestas cient√≠ficas y visuales.
        """
        self.model_id = model_id
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.is_loaded = False
        
        # ------------------------------------------------------------------
        # DICCIONARIOS PARA FILTRO R√ÅPIDO (FAST PATH)
        # Evitan usar la GPU para saludos simples
        # ------------------------------------------------------------------
        self.GREETINGS = {
            'hola', 'buenos dias', 'buenas tardes', 'buenas noches', 
            'hi', 'hello', 'holi', 'que tal', 'saludos'
        }
        self.FAREWELLS = {
            'adios', 'chao', 'hasta luego', 'bye', 'nos vemos', 
            'gracias', 'muchas gracias', 'ok gracias'
        }
    
    def load_model(self):
        """
        Carga el modelo LLaVA en memoria (float16 para ahorrar VRAM)
        """
        try:
            print(f">>> Cargando modelo LLaVA (Modo Experto) desde {self.model_id}...")
            
            self.model = LlavaForConditionalGeneration.from_pretrained(
                self.model_id, 
                torch_dtype=torch.float16, 
                low_cpu_mem_usage=True,
            ).to(self.device)
            
            self.processor = AutoProcessor.from_pretrained(self.model_id)
            
            self.is_loaded = True
            print("‚úì Modelo LLaVA cargado exitosamente")
            return True, "Modelo LLaVA cargado correctamente"
            
        except Exception as e:
            self.is_loaded = False
            error_msg = f"Error al cargar LLaVA: {str(e)}"
            print(f"‚úó {error_msg}")
            return False, error_msg

    def _check_conversational_intent(self, text):
        """
        Ruta R√°pida (CPU): Detecta intenciones simples antes de invocar al modelo pesado.
        Retorna: str (respuesta) o None (si requiere an√°lisis visual)
        """
        if not text: return None
        
        # Limpieza: min√∫sculas y quitar puntuaci√≥n b√°sica
        clean_text = re.sub(r'[^\w\s]', '', text.lower()).strip()
        words = clean_text.split()
        
        # Si el mensaje es largo (> 5 palabras), asumimos que es una consulta t√©cnica
        # Ejemplo: "Hola, ¬øpor qu√© mi planta tiene hojas amarillas?" -> Pasa al modelo
        if len(words) > 5:
            return None
            
        # Verificar saludos
        if any(word in self.GREETINGS for word in words):
            return (
                "üëã **¬°Hola! Soy Dr. agro.**\n\n"
                "Estoy listo para ayudarte. Por favor, sube una imagen clara de la hoja, "
                "tallo o fruto afectado y har√© un diagn√≥stico t√©cnico inmediato."
            )
            
        # Verificar despedidas
        if any(word in self.FAREWELLS for word in words):
            return (
                "ü§ù **¬°Hasta luego!**\n\n"
                "Recuerda monitorear tus cultivos frecuentemente. "
                "Estar√© aqu√≠ si necesitas otra opini√≥n t√©cnica."
            )
            
        return None
    
    def analyze_image(self, image, question=None, max_new_tokens=750):
        """
        Analiza una imagen aplicando el System Prompt cient√≠fico y formato Markdown.
        """
        if not self.is_loaded:
            return False, "Error: Modelo no cargado. Llama a load_model() primero."
        
        # ---------------------------------------------------------
        # 1. FAST PATH: Verificar si es solo un saludo
        # ---------------------------------------------------------
        intent_response = self._check_conversational_intent(question)
        if intent_response:
            return True, intent_response
        
        # ---------------------------------------------------------
        # 2. SLOW PATH: An√°lisis Neuronal Profundo
        # ---------------------------------------------------------
        try:
            # Procesar imagen
            if isinstance(image, str):
                image = Image.open(image).convert('RGB')
            
            # Definir consulta base si est√° vac√≠a
            user_query = question if question else "Realiza un diagn√≥stico t√©cnico completo de esta planta."
            
            # ---------------------------------------------------------
            # PROMPT DE INGENIER√çA: Estructura Gemini + Persona Experta
            # ---------------------------------------------------------
            prompt_structure = (
                "TAREA: Act√∫a como un Fitopat√≥logo e Ingeniero Agr√≥nomo Senior. Analiza la IMAGEN ADJUNTA.\n"
                "IDIOMA DE SALIDA: Espa√±ol (Estrictamente).\n"
                "FORMATO: Usa Markdown para estructurar la respuesta visualmente. Sigue esta plantilla:\n\n"
                
                "### üî¨ Diagn√≥stico Identificado\n"
                "**Nombre Com√∫n:** [Nombre de la enfermedad/plaga]\n"
                "**Nombre Cient√≠fico:** *[G√©nero especie]* (Taxonom√≠a)\n"
                "**Nivel de Confianza:** [Alto/Medio/Bajo] basado en signos visuales.\n\n"
                
                "### üçÇ Sintomatolog√≠a Observada\n"
                "Describe t√©cnicamente los signos patol√≥gicos visibles en la imagen:\n"
                "* [Signo visual 1: ej. Clorosis, Necrosis, Halo]\n"
                "* [Signo visual 2: ej. Patr√≥n de manchas, Esporulaci√≥n]\n\n"
                
                "### ü¶† Etiolog√≠a (Causa Probable)\n"
                "**Tipo de Agente:** [Hongo / Bacteria / Virus / Insecto / Nutricional]\n"
                "[Explicaci√≥n breve del mecanismo de acci√≥n del pat√≥geno]\n\n"
                
                "### üõ°Ô∏è Recomendaciones de Manejo Integrado\n"
                "1. [Acci√≥n Cultural: ej. Poda, Riego]\n"
                "2. [Acci√≥n Qu√≠mica/Biol√≥gica sugerida]\n"
                "3. [Medida preventiva]\n\n"
                
                f"CONSULTA ESPEC√çFICA DEL USUARIO: {user_query}"
            )

            # Construcci√≥n de la conversaci√≥n
            # NOTA: Ponemos la imagen PRIMERO para asegurar atenci√≥n visual
            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": prompt_structure},
                    ],
                },
            ]
            
            prompt = self.processor.apply_chat_template(
                conversation, 
                add_generation_prompt=True
            )
            
            inputs = self.processor(
                images=image, 
                text=prompt, 
                return_tensors='pt'
            ).to(self.device, torch.float16)
            
            # Generaci√≥n con par√°metros ajustados para rigor cient√≠fico
            with torch.inference_mode():
                output = self.model.generate(
                    **inputs, 
                    max_new_tokens=max_new_tokens, 
                    do_sample=True,
                    temperature=0.2,       # Baja creatividad para seguir la plantilla
                    top_p=0.9,
                    repetition_penalty=1.15 # Evitar bucles en descripciones largas
                )
            
            # Decodificaci√≥n y Limpieza
            response = self.processor.decode(output[0][2:], skip_special_tokens=True)
            
            if "ASSISTANT:" in response:
                response = response.split("ASSISTANT:")[-1].strip()
            
            # Verificaci√≥n de fallo de visi√≥n (com√∫n en LLaVA)
            if "proporci√≥name una imagen" in response.lower() or "no veo imagen" in response.lower():
                 return True, "‚ö†Ô∏è **Atenci√≥n:** El modelo no pudo enfocar correctamente la imagen. Por favor intenta:\n1. Recortar la imagen para centrar la hoja/fruto.\n2. Usar una foto con mejor iluminaci√≥n."
            
            # Liberar tensores de entrada inmediatamente
            del inputs
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return True, response
            
        except Exception as e:
            error_msg = f"Error cr√≠tico en an√°lisis visual: {str(e)}"
            print(f"‚úó {error_msg}")
            return False, error_msg
    
    def unload_model(self):
        """
        Descarga el modelo completamente para liberar VRAM para otros procesos (Video/RAG)
        """
        try:
            if self.model is not None:
                del self.model
                self.model = None
            
            if self.processor is not None:
                del self.processor
                self.processor = None
            
            # Forzar recolecci√≥n de basura de Python y CUDA
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.is_loaded = False
            print("‚úì Modelo LLaVA descargado de memoria")
            return True, "Modelo descargado correctamente"
            
        except Exception as e:
            error_msg = f"Error al descargar modelo: {str(e)}"
            return False, error_msg

# ==========================================
# BLOQUE DE PRUEBA (Solo se ejecuta si corres este archivo directamente)
# ==========================================
if __name__ == "__main__":
    print("--- Test de Llava_LDD ---")
    detector = LlavaPlantDiseaseDetector()
    
    # Prueba de Fast Path (Sin cargar modelo)
    print("Probando saludo:", detector._check_conversational_intent("Hola buenos dias"))
    
    # Cargar y Probar IA
    success, msg = detector.load_model()
    if success:
        # Reemplaza con una ruta real de tu PC para probar
        ruta_imagen = "test_leaf.jpg" 
        try:
            # Crear imagen dummy si no existe para evitar error en test
            img = Image.new('RGB', (100, 100), color = 'green')
            
            print("\nAnalizando imagen simulada...")
            ok, resp = detector.analyze_image(img, "Diagnostica esto")
            print("\nRESPUESTA GENERADA:\n")
            print(resp)
        except Exception as e:
            print(e)
        
        detector.unload_model()